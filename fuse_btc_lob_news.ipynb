{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LOB: E:\\DL Project\\data\\lob.csv\n",
      "LOB window: 2023-01-09 22:17:40.926000+00:00 -> 2023-01-20 18:10:48.672000+00:00 rows=3,730,870\n"
     ]
    }
   ],
   "source": [
    "# --------------------- 1) LOAD + PARSE LOB (robust) ---------------------\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "if not LOB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"LOB file not found: {LOB_PATH}\")\n",
    "\n",
    "print(f\"Loading LOB: {LOB_PATH}\")\n",
    "lob = pd.read_csv(LOB_PATH)\n",
    "\n",
    "# drop index column if present\n",
    "if \"Unnamed: 0\" in lob.columns:\n",
    "    lob = lob.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# 1) Prefer a Unix-ms column (often named \"0\") -> 13-digit integers\n",
    "ms_cols = [c for c in lob.columns\n",
    "           if lob[c].astype(str).str.fullmatch(r\"\\d{13}\").mean() > 0.8]\n",
    "\n",
    "if ms_cols:\n",
    "    # use the first ms column found\n",
    "    ms_col = ms_cols[0]\n",
    "    lob[\"ts\"] = pd.to_datetime(lob[ms_col].astype(\"int64\"), unit=\"ms\", utc=True)\n",
    "else:\n",
    "    # 2) Else, look for a single column that already parses as full datetime\n",
    "    dt_cols = []\n",
    "    for c in lob.columns[:10]:\n",
    "        parsed = pd.to_datetime(lob[c].astype(str), errors=\"coerce\", utc=True)\n",
    "        if parsed.notna().mean() > 0.8:\n",
    "            dt_cols.append(c)\n",
    "    if dt_cols:\n",
    "        lob[\"ts\"] = pd.to_datetime(lob[dt_cols[0]].astype(str), errors=\"coerce\", utc=True)\n",
    "    else:\n",
    "        # 3) Else, detect separate date + time columns and combine\n",
    "        def looks_like_date(s: pd.Series): \n",
    "            t = s.astype(str).head(20)\n",
    "            return t.str.contains(r\"^\\d{4}-\\d{2}-\\d{2}$\").mean() > 0.6\n",
    "        def looks_like_time(s: pd.Series):\n",
    "            t = s.astype(str).head(20)\n",
    "            return t.str.contains(r\"^\\d{2}:\\d{2}:\\d{2}$\").mean() > 0.6\n",
    "\n",
    "        date_col = next((c for c in lob.columns[:10] if looks_like_date(lob[c])), None)\n",
    "        time_col = next((c for c in lob.columns[:10] if looks_like_time(lob[c])), None)\n",
    "        if not date_col or not time_col:\n",
    "            raise KeyError(\"Could not detect timestamp: no unix-ms, full datetime, or separate date+time columns.\")\n",
    "        lob[\"ts\"] = pd.to_datetime(\n",
    "            lob[date_col].astype(str) + \" \" + lob[time_col].astype(str),\n",
    "            utc=True, errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "# keep 2023 (your LOB window)\n",
    "lob = lob.sort_values(\"ts\").reset_index(drop=True)\n",
    "lob = lob[(lob[\"ts\"] >= \"2023-01-01\") & (lob[\"ts\"] <= \"2023-12-31 23:59:59\")]\n",
    "\n",
    "print(\"LOB window:\", lob[\"ts\"].min(), \"->\", lob[\"ts\"].max(), f\"rows={len(lob):,}\")\n",
    "\n",
    "# book columns start after the first 3 meta columns in your file\n",
    "start_col_index = 3  # [0]=ms, [1]=date/full-dt, [2]=time/full-dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02467e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LOB: E:\\DL Project\\data\\lob.csv\n",
      "LOB window: 2023-01-09 22:17:40.926000+00:00 -> 2023-01-20 18:10:48.672000+00:00 rows=3,730,870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ONGC\\AppData\\Local\\Temp\\ipykernel_1520\\818954630.py:160: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  lob_1m[\"micro_ret_1\"] = lob_1m[\"micro_last\"].pct_change()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOB_1m rows: 15593 2023-01-09 22:17:00 → 2023-01-20 18:10:00\n",
      "Loading News: E:\\DL Project\\data\\news\\bitcoin_sentiments_21_24.csv\n",
      "News rows in range: 11158 2022-01-01 20:57:00+00:00 → 2024-09-12 00:00:00+00:00\n",
      "News minute buckets: 9831 2022-01-01 20:58:00+00:00 → 2024-09-12 00:01:00+00:00\n",
      "\n",
      "Saved 15,588 rows → E:\\DL Project\\outputs\\btc_lob_news_1min.parquet\n",
      "                           mid_last  sent_mean  news_count   y_ret_5m  \\\n",
      "ts                                                                      \n",
      "2023-01-20 18:03:00+00:00    1.5570        0.0         0.0  14.530186   \n",
      "2023-01-20 18:04:00+00:00    7.1195        0.0         0.0   1.969169   \n",
      "2023-01-20 18:05:00+00:00    1.4460        0.0         0.0   0.266252   \n",
      "\n",
      "                           y_dir_5m  \n",
      "ts                                   \n",
      "2023-01-20 18:03:00+00:00       1.0  \n",
      "2023-01-20 18:04:00+00:00       1.0  \n",
      "2023-01-20 18:05:00+00:00       1.0  \n"
     ]
    }
   ],
   "source": [
    "# === Bitcoin LOB + News Fusion (robust, Windows paths, UTC-safe) ===\n",
    "# Install once:\n",
    "#   pip install pandas numpy pyarrow fastparquet vaderSentiment\n",
    "# Optional (better sentiment but slower):\n",
    "#   pip install transformers torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --------------------- CONFIG ---------------------\n",
    "LOB_PATH  = Path(r\"E:\\DL Project\\data\\lob.csv\")  # your LOB CSV (Jan 9–20, 2023)\n",
    "NEWS_PATH = Path(r\"E:\\DL Project\\data\\news\\bitcoin_sentiments_21_24.csv\")\n",
    "OUT_PATH  = Path(r\"E:\\DL Project\\outputs\\btc_lob_news_1min.parquet\")\n",
    "\n",
    "BUCKET    = \"1min\"   # resampling grid\n",
    "LATENCY_S = 60       # news must be >=60s old by bucket end (anti-leakage)\n",
    "LABEL_H   = 5        # 5-min ahead label\n",
    "\n",
    "DATE_START = pd.Timestamp(\"2022-01-01\", tz=\"UTC\")\n",
    "DATE_END   = pd.Timestamp(\"2024-12-31 23:59:59\", tz=\"UTC\")\n",
    "\n",
    "USE_FINBERT   = False\n",
    "FINBERT_MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "# If your LOB has 3 meta columns then [ask_px, ask_sz, bid_px, bid_sz]*levels starts at index 3:\n",
    "START_COL_INDEX = 3\n",
    "TOP_K_LEVELS    = 10\n",
    "\n",
    "# --------------------- HELPERS ---------------------\n",
    "def to_dt_utc(x):\n",
    "    \"\"\"Parse timestamps robustly to UTC pandas.Timestamp.\"\"\"\n",
    "    if pd.isna(x): \n",
    "        return pd.NaT\n",
    "    try:\n",
    "        xi = int(x)\n",
    "        unit = \"ms\" if xi > 1e12 else \"s\"\n",
    "        return pd.to_datetime(xi, unit=unit, utc=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(x, utc=True, errors=\"coerce\")\n",
    "\n",
    "def detect_date_time_cols(df, scan=12):\n",
    "    \"\"\"Return (date_col, time_col) if present.\"\"\"\n",
    "    date_col = None\n",
    "    time_col = None\n",
    "    for c in df.columns[:scan]:\n",
    "        s = df[c].astype(str).head(50)\n",
    "        if s.str.contains(r\"^\\d{4}-\\d{2}-\\d{2}$\", regex=True).mean() > 0.7:\n",
    "            date_col = c; break\n",
    "    for c in df.columns[:scan]:\n",
    "        s = df[c].astype(str).head(50)\n",
    "        if s.str.contains(r\"^\\d{2}:\\d{2}:\\d{2}$\", regex=True).mean() > 0.7:\n",
    "            time_col = c; break\n",
    "    return date_col, time_col\n",
    "\n",
    "def build_tick_feats_from_pairs(df, k=10, start_col_index=3):\n",
    "    \"\"\"\n",
    "    Your LOB rows repeat [ask_px, ask_sz, bid_px, bid_sz] per level after the first meta columns.\n",
    "    \"\"\"\n",
    "    if df.shape[1] <= start_col_index:\n",
    "        raise ValueError(\"Not enough columns in LOB to parse order book levels. Check start_col_index.\")\n",
    "\n",
    "    arr = df.iloc[:, start_col_index:].to_numpy(dtype=float)\n",
    "    n_levels = min(arr.shape[1] // 4, k)\n",
    "    if n_levels < 1:\n",
    "        raise ValueError(\"Could not detect any price/size level blocks. Check your CSV structure.\")\n",
    "\n",
    "    apx = arr[:, 0::4][:, :n_levels]  # ask prices\n",
    "    asz = arr[:, 1::4][:, :n_levels]  # ask sizes\n",
    "    bpx = arr[:, 2::4][:, :n_levels]  # bid prices\n",
    "    bsz = arr[:, 3::4][:, :n_levels]  # bid sizes\n",
    "\n",
    "    best_bid = bpx[:, 0]\n",
    "    best_ask = apx[:, 0]\n",
    "    mid = (best_bid + best_ask) / 2.0\n",
    "    spread = (best_ask - best_bid)\n",
    "    spread_bps = 1e4 * spread / np.where(mid == 0, np.nan, mid)\n",
    "    imb10 = (bsz.sum(axis=1) - asz.sum(axis=1)) / (bsz.sum(axis=1) + asz.sum(axis=1) + 1e-9)\n",
    "    micro = (best_ask*bsz[:,0] + best_bid*asz[:,0]) / (bsz[:,0] + asz[:,0] + 1e-9)\n",
    "    depth_sum = bsz.sum(axis=1) + asz.sum(axis=1)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mid\": mid,\n",
    "        \"spread_bps\": spread_bps,\n",
    "        \"imb10\": imb10,\n",
    "        \"micro\": micro,\n",
    "        \"depth_sum\": depth_sum,\n",
    "    }, index=df.index)\n",
    "    # simple OFI proxy\n",
    "    out[\"ofi\"] = pd.Series(out[\"micro\"]).diff().fillna(0).values\n",
    "    out[\"ts\"]  = df[\"ts\"].values\n",
    "    return out\n",
    "\n",
    "def to_utc_index(idx):\n",
    "    \"\"\"Ensure a DatetimeIndex is tz-aware UTC.\"\"\"\n",
    "    idx = pd.to_datetime(idx, errors=\"coerce\")\n",
    "    if getattr(idx, \"tz\", None) is None:\n",
    "        return idx.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        return idx.tz_convert(\"UTC\")\n",
    "\n",
    "# --------------------- 1) LOAD + PARSE LOB ---------------------\n",
    "if not LOB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"LOB file not found: {LOB_PATH}\")\n",
    "\n",
    "print(f\"Loading LOB: {LOB_PATH}\")\n",
    "lob = pd.read_csv(LOB_PATH)\n",
    "\n",
    "# Drop obvious index columns if present\n",
    "for col in [\"Unnamed: 0\", \"index\"]:\n",
    "    if col in lob.columns:\n",
    "        lob = lob.drop(columns=[col])\n",
    "\n",
    "# Prefer a Unix-ms column if present (13-digit numeric-like)\n",
    "ms_cols = [c for c in lob.columns if lob[c].astype(str).str.fullmatch(r\"\\d{13}\").mean() > 0.8]\n",
    "if ms_cols:\n",
    "    ms_col = ms_cols[0]\n",
    "    lob[\"ts\"] = pd.to_datetime(lob[ms_col].astype(\"int64\"), unit=\"ms\", utc=True)\n",
    "else:\n",
    "    # Full datetime column?\n",
    "    dt_cols = []\n",
    "    for c in lob.columns[:12]:\n",
    "        parsed = pd.to_datetime(lob[c].astype(str), errors=\"coerce\", utc=True)\n",
    "        if parsed.notna().mean() > 0.8:\n",
    "            dt_cols.append(c)\n",
    "    if dt_cols:\n",
    "        lob[\"ts\"] = pd.to_datetime(lob[dt_cols[0]].astype(str), errors=\"coerce\", utc=True)\n",
    "    else:\n",
    "        # Separate date + time columns\n",
    "        date_col, time_col = detect_date_time_cols(lob)\n",
    "        if not date_col or not time_col:\n",
    "            raise KeyError(\"Could not detect timestamp (no unix-ms, full datetime, or separate date+time).\")\n",
    "        lob[\"ts\"] = pd.to_datetime(\n",
    "            lob[date_col].astype(str) + \" \" + lob[time_col].astype(str),\n",
    "            utc=True, errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "# Keep 2023 (your LOB window)\n",
    "lob = lob.sort_values(\"ts\").reset_index(drop=True)\n",
    "lob = lob[(lob[\"ts\"] >= \"2023-01-01\") & (lob[\"ts\"] <= \"2023-12-31 23:59:59\")]\n",
    "print(\"LOB window:\", lob[\"ts\"].min(), \"->\", lob[\"ts\"].max(), f\"rows={len(lob):,}\")\n",
    "\n",
    "# Build tick features from your column layout\n",
    "lob_tick = build_tick_feats_from_pairs(lob, k=TOP_K_LEVELS, start_col_index=START_COL_INDEX)\\\n",
    "            .dropna(subset=[\"mid\",\"micro\"]).sort_values(\"ts\")\n",
    "\n",
    "# Resample to minute\n",
    "g = lob_tick.set_index(\"ts\").resample(BUCKET)\n",
    "lob_1m = pd.DataFrame({\n",
    "    \"mid_last\":        g[\"mid\"].last(),\n",
    "    \"mid_mean\":        g[\"mid\"].mean(),\n",
    "    \"spread_bps_mean\": g[\"spread_bps\"].mean(),\n",
    "    \"imb10_mean\":      g[\"imb10\"].mean(),\n",
    "    \"micro_last\":      g[\"micro\"].last(),\n",
    "    \"ofi_sum\":         g[\"ofi\"].sum(),\n",
    "    \"depth_sum_mean\":  g[\"depth_sum\"].mean(),\n",
    "})\n",
    "lob_1m[\"micro_ret_1\"] = lob_1m[\"micro_last\"].pct_change()\n",
    "lob_1m = lob_1m.dropna(subset=[\"mid_last\"]).sort_index()\n",
    "print(\"LOB_1m rows:\", len(lob_1m), lob_1m.index.min(), \"→\", lob_1m.index.max())\n",
    "\n",
    "# --------------------- 2) LOAD + PREP NEWS ---------------------\n",
    "if not NEWS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"News file not found: {NEWS_PATH}\")\n",
    "\n",
    "print(f\"Loading News: {NEWS_PATH}\")\n",
    "news = pd.read_csv(NEWS_PATH)\n",
    "\n",
    "# Detect datetime column in news\n",
    "dt_cols = [c for c in news.columns if re.search(r\"date|time|publish|created|datetime\", c, re.I)]\n",
    "if not dt_cols:\n",
    "    raise KeyError(\"No datetime-like column found in NEWS CSV. Rename or point to the correct column.\")\n",
    "news[\"published_dt\"] = news[dt_cols[0]].apply(to_dt_utc)\n",
    "\n",
    "# Build text column robustly (title + body, or any available text-like columns)\n",
    "title_col = next((c for c in news.columns if re.search(r\"title|headline\", c, re.I)), None)\n",
    "body_col  = next((c for c in news.columns if re.search(r\"body|content|article|summary|text\", c, re.I)), None)\n",
    "\n",
    "if title_col and body_col:\n",
    "    news[\"text\"] = (news[title_col].astype(str).fillna(\"\") + \" \" +\n",
    "                    news[body_col].astype(str).fillna(\"\")).str.strip()\n",
    "elif title_col:\n",
    "    news[\"text\"] = news[title_col].astype(str).fillna(\"\").str.strip()\n",
    "elif body_col:\n",
    "    news[\"text\"] = news[body_col].astype(str).fillna(\"\").str.strip()\n",
    "else:\n",
    "    str_cols = [c for c in news.columns if news[c].dtype == \"object\"]\n",
    "    if not str_cols:\n",
    "        raise KeyError(\"No textual columns found in news CSV.\")\n",
    "    news[\"text\"] = news[str_cols].astype(str).agg(\" \".join, axis=1).str.strip()\n",
    "\n",
    "# Filter date range + clean\n",
    "news = news.dropna(subset=[\"published_dt\"])\n",
    "news = news[(news[\"published_dt\"] >= DATE_START) & (news[\"published_dt\"] <= DATE_END)]\n",
    "news = news[news[\"text\"].str.len() > 0].copy().sort_values(\"published_dt\")\n",
    "print(\"News rows in range:\", len(news), news[\"published_dt\"].min(), \"→\", news[\"published_dt\"].max())\n",
    "\n",
    "# --------------------- 3) SENTIMENT ---------------------\n",
    "if USE_FINBERT:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL); mdl.eval()\n",
    "    def finbert_score(t):\n",
    "        t = (t or \"\")[:1500]\n",
    "        with torch.no_grad():\n",
    "            enc = tok(t, return_tensors=\"pt\", truncation=True)\n",
    "            p = torch.softmax(mdl(**enc).logits, dim=-1).cpu().numpy()[0]  # [neg,neu,pos]\n",
    "        return float(p[2] - p[0])  # pos - neg\n",
    "    news[\"sent\"] = news[\"text\"].map(finbert_score)\n",
    "else:\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    news[\"sent\"] = news[\"text\"].map(lambda t: vader.polarity_scores(t)[\"compound\"])\n",
    "\n",
    "# Anti-leakage latency and bucketing\n",
    "news[\"effective_dt\"] = (news[\"published_dt\"] + pd.to_timedelta(LATENCY_S, unit=\"s\")).dt.floor(BUCKET)\n",
    "\n",
    "news_agg = news.groupby(\"effective_dt\").agg(\n",
    "    sent_mean=(\"sent\",\"mean\"),\n",
    "    sent_std =(\"sent\",\"std\"),\n",
    "    sent_pos_share=(\"sent\", lambda s: float(np.mean(s > 0))),\n",
    "    news_count=(\"sent\",\"count\"),\n",
    ").rename_axis(\"ts\").sort_index()\n",
    "news_agg[\"sent_std\"] = news_agg[\"sent_std\"].fillna(0)\n",
    "print(\"News minute buckets:\", len(news_agg), news_agg.index.min(), \"→\", news_agg.index.max())\n",
    "\n",
    "# --------------------- 4) NORMALIZE INDICES TO UTC + JOIN + LABELS ---------------------\n",
    "lob_1m.index   = to_utc_index(lob_1m.index)\n",
    "news_agg.index = to_utc_index(news_agg.index)\n",
    "\n",
    "X = lob_1m.join(\n",
    "    news_agg,\n",
    "    how=\"left\"\n",
    ").fillna({\"sent_mean\":0, \"sent_std\":0, \"sent_pos_share\":0, \"news_count\":0})\n",
    "\n",
    "# Labels (5-min ahead)\n",
    "H = LABEL_H\n",
    "X[\"y_ret_5m\"] = X[\"mid_last\"].shift(-H)/X[\"mid_last\"] - 1\n",
    "X[\"y_dir_5m\"] = np.sign(X[\"y_ret_5m\"]).clip(-1, 1)\n",
    "\n",
    "dataset = X.dropna(subset=[\"y_ret_5m\"]).copy()\n",
    "\n",
    "# --------------------- 5) SAVE ---------------------\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "dataset.to_parquet(OUT_PATH)\n",
    "print(f\"\\nSaved {len(dataset):,} rows → {OUT_PATH}\")\n",
    "print(dataset.tail(3)[[\"mid_last\",\"sent_mean\",\"news_count\",\"y_ret_5m\",\"y_dir_5m\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0478ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns (first 24): ['Unnamed: 0', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1673302660926</td>\n",
       "      <td>2023-01-09 22:17:40</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>23.371</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.746</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>5.168</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.692</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>1.904</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>2.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1673302661177</td>\n",
       "      <td>2023-01-09 22:17:41</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>24.232</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.694</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>6.043</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.001</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>1.869</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>2.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1673302661427</td>\n",
       "      <td>2023-01-09 22:17:41</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>24.403</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.694</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>6.043</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.012</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>1.869</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>2.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1673302661678</td>\n",
       "      <td>2023-01-09 22:17:41</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>24.874</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.694</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>6.043</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.001</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>2.570</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>2.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1673302661928</td>\n",
       "      <td>2023-01-09 22:17:41</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>24.403</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.694</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>6.043</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.001</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>2.589</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>2.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1673302662178</td>\n",
       "      <td>2023-01-09 22:17:42</td>\n",
       "      <td>17181.6</td>\n",
       "      <td>24.403</td>\n",
       "      <td>17181.5</td>\n",
       "      <td>0.686</td>\n",
       "      <td>17181.4</td>\n",
       "      <td>5.428</td>\n",
       "      <td>17181.2</td>\n",
       "      <td>...</td>\n",
       "      <td>17182.2</td>\n",
       "      <td>6.043</td>\n",
       "      <td>17182.3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17182.4</td>\n",
       "      <td>6.001</td>\n",
       "      <td>17182.5</td>\n",
       "      <td>3.785</td>\n",
       "      <td>17182.6</td>\n",
       "      <td>1.404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              0                    1        2       3        4  \\\n",
       "0           0  1673302660926  2023-01-09 22:17:40  17181.6  23.371  17181.5   \n",
       "1           1  1673302661177  2023-01-09 22:17:41  17181.6  24.232  17181.5   \n",
       "2           2  1673302661427  2023-01-09 22:17:41  17181.6  24.403  17181.5   \n",
       "3           3  1673302661678  2023-01-09 22:17:41  17181.6  24.874  17181.5   \n",
       "4           4  1673302661928  2023-01-09 22:17:41  17181.6  24.403  17181.5   \n",
       "5           5  1673302662178  2023-01-09 22:17:42  17181.6  24.403  17181.5   \n",
       "\n",
       "       5        6      7        8  ...       32     33       34    35  \\\n",
       "0  0.746  17181.4  5.428  17181.2  ...  17182.2  5.168  17182.3  0.02   \n",
       "1  0.694  17181.4  5.428  17181.2  ...  17182.2  6.043  17182.3  0.02   \n",
       "2  0.694  17181.4  5.428  17181.2  ...  17182.2  6.043  17182.3  0.02   \n",
       "3  0.694  17181.4  5.428  17181.2  ...  17182.2  6.043  17182.3  0.02   \n",
       "4  0.694  17181.4  5.428  17181.2  ...  17182.2  6.043  17182.3  0.02   \n",
       "5  0.686  17181.4  5.428  17181.2  ...  17182.2  6.043  17182.3  0.02   \n",
       "\n",
       "        36     37       38     39       40     41  \n",
       "0  17182.4  6.692  17182.5  1.904  17182.6  2.546  \n",
       "1  17182.4  6.001  17182.5  1.869  17182.6  2.105  \n",
       "2  17182.4  6.012  17182.5  1.869  17182.6  2.713  \n",
       "3  17182.4  6.001  17182.5  2.570  17182.6  2.613  \n",
       "4  17182.4  6.001  17182.5  2.589  17182.6  2.591  \n",
       "5  17182.4  6.001  17182.5  3.785  17182.6  1.404  \n",
       "\n",
       "[6 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample raw numeric slice (first row):\n",
      "[0, 1673302660926, '2023-01-09 22:17:40', 17181.6, 23.371, 17181.5, 0.746, 17181.4, 5.428, 17181.2, 0.89, 17181.1, 3.787, 17181.0, 0.908, 17180.9, 1.628, 17180.8, 0.007, 17180.7, 0.876, 17180.6, 2.854, 17181.7]\n",
      "\n",
      "Assumed first-level columns: ['2', '3', '4', '5']\n",
      "First-level sample stats (200 rows):\n",
      "   count          mean        std        min        25%        50%  \\\n",
      "2  200.0  17183.546500   1.606891  17181.600  17181.800  17183.700   \n",
      "3  200.0     27.380875  21.076859      1.657      4.895     24.335   \n",
      "4  200.0  17183.446500   1.606891  17181.500  17181.700  17183.600   \n",
      "5  200.0      3.011620   3.584985      0.004      1.035      1.070   \n",
      "\n",
      "           75%        max  \n",
      "2  17185.20000  17185.200  \n",
      "3     50.74975     75.235  \n",
      "4  17185.10000  17185.100  \n",
      "5      3.82000     24.981  \n"
     ]
    }
   ],
   "source": [
    "# ---------- DIAGNOSTICS -----------\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "LOB_PATH = Path(r\"E:/DL Project/data/lob.csv\")\n",
    "lob = pd.read_csv(LOB_PATH)\n",
    "print(\"Columns (first 24):\", lob.columns.tolist()[:24])\n",
    "display(lob.head(6))\n",
    "\n",
    "# show first 3 meta cols + first 12 book cols as raw numbers\n",
    "print(\"\\nSample raw numeric slice (first row):\")\n",
    "print(lob.iloc[0, :24].to_list())\n",
    "\n",
    "# show some simple stats on first-level columns assuming start index 3\n",
    "start = 3\n",
    "cols = lob.columns.tolist()\n",
    "first_level = cols[start:start+4]\n",
    "print(\"\\nAssumed first-level columns:\", first_level)\n",
    "sample = lob.iloc[:200, start:start+4].astype(float)\n",
    "print(\"First-level sample stats (200 rows):\")\n",
    "print(sample.describe().T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
